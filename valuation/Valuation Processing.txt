module load python/gnu/3.6.5
module load spark/2.4.0
pyspark
data=spark.read.load("cleaned_data_2.csv", format="csv", inferschema="true", header="true")
data.createOrReplaceTempView('valdata')

#aggregate year over year data for each BBLE key
trend10_11=spark.sql('SELECT CASE WHEN yr_2010.BBLE is not null THEN yr_2010.BBLE ELSE yr_2011.BBLE END BBLE, \
yr_2010.FULLVAL FULLVAL_2010, yr_2011.FULLVAL FULLVAL_2011 FROM \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2010/11") yr_2010 FULL OUTER JOIN \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2011/12") yr_2011 ON yr_2010.BBLE=yr_2011.BBLE')
trend10_11.createOrReplaceTempView('trend10_11')

trend12_13=spark.sql('SELECT CASE WHEN yr_2012.BBLE is not null THEN yr_2012.BBLE ELSE yr_2013.BBLE END BBLE, \
yr_2012.FULLVAL FULLVAL_2012, yr_2013.FULLVAL FULLVAL_2013 FROM \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2012/13") yr_2012 FULL OUTER JOIN \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2013/14") yr_2013 ON yr_2012.BBLE=yr_2013.BBLE')
trend12_13.createOrReplaceTempView('trend12_13')

trend14_15=spark.sql('SELECT CASE WHEN yr_2014.BBLE is not null THEN yr_2014.BBLE ELSE yr_2015.BBLE END BBLE, \
yr_2014.FULLVAL FULLVAL_2014, yr_2015.FULLVAL FULLVAL_2015 FROM \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2014/15") yr_2014 FULL OUTER JOIN \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2015/16") yr_2015 ON yr_2014.BBLE=yr_2015.BBLE')
trend14_15.createOrReplaceTempView('trend14_15')

trend16_17=spark.sql('SELECT CASE WHEN yr_2016.BBLE is not null THEN yr_2016.BBLE ELSE yr_2017.BBLE END BBLE, \
yr_2016.FULLVAL FULLVAL_2016, yr_2017.FULLVAL FULLVAL_2017 FROM \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2016/17") yr_2016 FULL OUTER JOIN \
(SELECT BBLE, FULLVAL from valdata WHERE YEAR="2017/18") yr_2017 ON yr_2016.BBLE=yr_2017.BBLE')
trend16_17.createOrReplaceTempView('trend16_17')

trend10_11_12_13=spark.sql('SELECT CASE WHEN trend10_11.BBLE is not null THEN trend10_11.BBLE ELSE trend12_13.BBLE END BBLE, \
FULLVAL_2010, FULLVAL_2011, FULLVAL_2012, FULLVAL_2013 \
FROM trend10_11 FULL OUTER JOIN trend12_13 ON trend10_11.BBLE=trend12_13.BBLE')
trend10_11_12_13.createOrReplaceTempView('trend10_11_12_13')

trend14_15_16_17=spark.sql('SELECT CASE WHEN trend14_15.BBLE is not null THEN trend14_15.BBLE ELSE trend16_17.BBLE END BBLE, \
FULLVAL_2014, FULLVAL_2015, FULLVAL_2016, FULLVAL_2017 \
FROM trend14_15 FULL OUTER JOIN trend16_17 ON trend14_15.BBLE=trend16_17.BBLE')
trend14_15_16_17.createOrReplaceTempView('trend14_15_16_17')

trend10_17=spark.sql('SELECT CASE WHEN trend10_11_12_13.BBLE is not null THEN trend10_11_12_13.BBLE ELSE trend14_15_16_17.BBLE END BBLE, \
FULLVAL_2010, FULLVAL_2011, FULLVAL_2012, FULLVAL_2013, FULLVAL_2014, FULLVAL_2015, FULLVAL_2016, FULLVAL_2017 \
FROM trend10_11_12_13 FULL OUTER JOIN trend14_15_16_17 ON trend10_11_12_13.BBLE=trend14_15_16_17.BBLE')
trend10_17.createOrReplaceTempView('trend10_17')

#create final table of BBLE with year over year data
merged_yrs=spark.sql('SELECT CASE WHEN trend10_17.BBLE is not null THEN trend10_17.BBLE ELSE yr_2018.BBLE END BBLE, \
FULLVAL_2010, FULLVAL_2011, FULLVAL_2012, FULLVAL_2013, FULLVAL_2014, FULLVAL_2015, FULLVAL_2016, FULLVAL_2017, yr_2018.FULLVAL FULLVAL_2018 \
FROM trend10_17 FULL OUTER JOIN (SELECT BBLE, FULLVAL from valdata WHERE YEAR="2018/19") yr_2018 \
ON trend10_17.BBLE=yr_2018.BBLE')
merged_yrs.createOrReplaceTempView('merged_yrs')

#calculate year over year growth
growth=spark.sql('SELECT BBLE, FULLVAL_2010, FULLVAL_2011, FULLVAL_2012, FULLVAL_2013, FULLVAL_2014, FULLVAL_2015, FULLVAL_2016, FULLVAL_2017, FULLVAL_2018, \
CASE WHEN (FULLVAL_2010 is not null AND FULLVAL_2011 is not null) THEN ((FULLVAL_2011-FULLVAl_2010)/FULLVAL_2010) ELSE null END 10_11_growth, \
CASE WHEN (FULLVAL_2011 is not null AND FULLVAL_2012 is not null) THEN ((FULLVAL_2012-FULLVAl_2011)/FULLVAL_2011) ELSE null END 11_12_growth, \
CASE WHEN (FULLVAL_2012 is not null AND FULLVAL_2013 is not null) THEN ((FULLVAL_2013-FULLVAl_2012)/FULLVAL_2012) ELSE null END 12_13_growth, \
CASE WHEN (FULLVAL_2013 is not null AND FULLVAL_2014 is not null) THEN ((FULLVAL_2014-FULLVAl_2013)/FULLVAL_2013) ELSE null END 13_14_growth, \
CASE WHEN (FULLVAL_2014 is not null AND FULLVAL_2015 is not null) THEN ((FULLVAL_2015-FULLVAl_2014)/FULLVAL_2014) ELSE null END 14_15_growth, \
CASE WHEN (FULLVAL_2015 is not null AND FULLVAL_2016 is not null) THEN ((FULLVAL_2016-FULLVAl_2015)/FULLVAL_2015) ELSE null END 15_16_growth, \
CASE WHEN (FULLVAL_2016 is not null AND FULLVAL_2017 is not null) THEN ((FULLVAL_2017-FULLVAl_2016)/FULLVAL_2016) ELSE null END 16_17_growth, \
CASE WHEN (FULLVAL_2017 is not null AND FULLVAL_2018 is not null) THEN ((FULLVAL_2018-FULLVAl_2017)/FULLVAL_2017) ELSE null END 17_18_growth, \
CASE WHEN (FULLVAL_2010 is not null AND FULLVAL_2011 is not null) THEN 1 ELSE 0 END 10_11_countnotnull, \
CASE WHEN (FULLVAL_2011 is not null AND FULLVAL_2012 is not null) THEN 1 ELSE 0 END 11_12_countnotnull, \
CASE WHEN (FULLVAL_2012 is not null AND FULLVAL_2013 is not null) THEN 1 ELSE 0 END 12_13_countnotnull, \
CASE WHEN (FULLVAL_2013 is not null AND FULLVAL_2014 is not null) THEN 1 ELSE 0 END 13_14_countnotnull, \
CASE WHEN (FULLVAL_2014 is not null AND FULLVAL_2015 is not null) THEN 1 ELSE 0 END 14_15_countnotnull, \
CASE WHEN (FULLVAL_2015 is not null AND FULLVAL_2016 is not null) THEN 1 ELSE 0 END 15_16_countnotnull, \
CASE WHEN (FULLVAL_2016 is not null AND FULLVAL_2017 is not null) THEN 1 ELSE 0 END 16_17_countnotnull, \
CASE WHEN (FULLVAL_2017 is not null AND FULLVAL_2018 is not null) THEN 1 ELSE 0 END 17_18_countnotnull \
FROM merged_yrs')
growth.createOrReplaceTempView('growth')

#calculate year over year growth averages per BBLE
avg_growth=spark.sql('select BBLE, FULLVAL_2010, FULLVAL_2011, FULLVAL_2012, FULLVAL_2013, FULLVAL_2014, FULLVAL_2015, FULLVAL_2016, \
FULLVAL_2017, FULLVAL_2018, 10_11_growth, 11_12_growth, 12_13_growth, 13_14_growth, 14_15_growth, 15_16_growth, 16_17_growth, 17_18_growth, \
(IFNULL(10_11_growth,0)+IFNULL(11_12_growth,0)+IFNULL(12_13_growth,0)+IFNULL(13_14_growth,0)+IFNULL(14_15_growth,0)+IFNULL(15_16_growth,0)+\
IFNULL(16_17_growth,0)+IFNULL(17_18_growth,0))/(10_11_countnotnull+11_12_countnotnull\
+12_13_countnotnull+13_14_countnotnull+14_15_countnotnull+15_16_countnotnull+16_17_countnotnull+17_18_countnotnull) growth_avg FROM growth')
avg_growth.createOrReplaceTempView('growth_rate')

#convert boro codes to names, year to int version
bble_zip=spark.sql('SELECT BBLE, POSTCODE, BORO, CASE WHEN BORO=1 THEN "Manhattan" \
WHEN BORO=2 THEN "Bronx" \
WHEN BORO=3 THEN "Brooklyn" \
WHEN BORO=4 THEN "Queens" \
WHEN BORO=5 THEN "Staten Island" END boro_name, \
CASE WHEN YEAR="2010/11" THEN 2010 \
WHEN YEAR="2011/12" THEN 2011 \
WHEN YEAR="2012/13" THEN 2012 \
WHEN YEAR="2013/14" THEN 2013 \
WHEN YEAR="2014/15" THEN 2014 \
WHEN YEAR="2015/16" THEN 2015 \
WHEN YEAR="2016/17" THEN 2016 \
WHEN YEAR="2017/18" THEN 2017 \
WHEN YEAR="2018/19" THEN 2018 END year_int FROM valdata')
bble_zip.createOrReplaceTempView('bble_zip')

#over time same BBLE key could have more than 1 zip value, use most recent zip categorization so each BBLE has 1 growth record only
zip_unique=spark.sql('SELECT bble_zip.BBLE, bble_zip.POSTCODE, bble_zip.BORO, bble_zip.boro_name \
FROM (SELECT BBLE, max(year_int) max_year FROM bble_zip GROUP BY BBLE) x \
LEFT JOIN bble_zip ON x.BBLE=bble_zip.BBLE AND bble_zip.year_int=x.max_year')
zip_unique.createOrReplaceTempView('zip_unique')

#get BBLE level growth by zip
growth_wzip=spark.sql('select growth_rate.BBLE, zip_unique.BORO, zip_unique.boro_name, zip_unique.POSTCODE, growth_rate.growth_avg \
FROM growth_rate JOIN zip_unique ON growth_rate.BBLE=zip_unique.BBLE')
growth_wzip.createOrReplaceTempView('final_growth')

#get BBLE growth by zip and year
growth_wzip_yr=spark.sql('SELECT growth_rate.BBLE, zip_unique.BORO, zip_unique.boro_name, zip_unique.POSTCODE, growth_rate.growth_avg, \
10_11_growth, 11_12_growth, 12_13_growth, 13_14_growth, 14_15_growth, 15_16_growth, 16_17_growth, 17_18_growth \
FROM growth_rate JOIN zip_unique ON growth_rate.BBLE=zip_unique.BBLE')
growth_wzip_yr.createOrReplaceTempView('growth_wzip_yr')

#get zip-level yearly growth
yearly_data=spark.sql('SELECT boro, boro_name, postcode, avg(10_11_growth) 10_11growth, avg(11_12_growth) 11_12growth, \
avg(12_13_growth) 12_13growth, avg(13_14_growth) 13_14growth, avg(14_15_growth) 14_15growth, \
avg(15_16_growth) 15_16growth, avg(16_17_growth) 16_17growth, avg(17_18_growth) 17_18growth \
FROM growth_wzip_yr WHERE postcode!="10103" AND postcode!="11227" AND postcode!="11359" GROUP BY boro, boro_name, postcode')
yearly_data.createOrReplaceTempView('yearly_data')

#get zip-level growth for entire time period
final_data=spark.sql('SELECT boro, boro_name, postcode, avg(growth_avg) avg_growth FROM final_growth WHERE growth_avg<15 \
GROUP BY boro, boro_name, postcode ORDER BY avg_growth DESC')
final_data.createOrReplaceTempView('final_data')

'''
by zip categorization version using standard dev from mean, not used settled on percentile instead
from pyspark.sql.functions import stddev_pop
from pyspark.sql.functions import percent_rank
mean=spark.sql('SELECT avg(avg_growth) avg from final_data').collect()[0][0]
std_dev=spark.sql('SELECT stddev_pop(avg_growth) std_dev from final_data').collect()[0][0]
final_wcat_stdev=spark.sql('SELECT postcode, avg_growth, {0} totalzip_mean, {1} totalzip_stddev, \
CASE WHEN avg_growth<({0}-({1}*2)) THEN "poor" \
WHEN avg_growth<{0} THEN "fair" WHEN avg_growth>({0}+({1}*2)) THEN "excellent" ELSE "good" END categorization \
FROM final_data'.format(mean,std_dev))
'''

#by zip categorization version using percentiles
summary=final_data.summary()
summary.createOrReplaceTempView("summary")
per_25=spark.sql('SELECT avg_growth from summary WHERE summary="25%"').collect()[0][0]
per_50=spark.sql('SELECT avg_growth from summary WHERE summary="50%"').collect()[0][0]
per_75=spark.sql('SELECT avg_growth from summary WHERE summary="75%"').collect()[0][0]
final_wcat=spark.sql('SELECT boro, boro_name,postcode, avg_growth, \
CASE WHEN avg_growth<{0} THEN "poor" WHEN avg_growth<{1} THEN "fair" \
WHEN avg_growth<{2} THEN "good" ELSE "excellent" END categorization \
FROM final_data'.format(per_25,per_50,per_75))

#save by zip categorization using percentiles
final_wcat_tosave=final_wcat.coalesce(1)
final_wcat_tosave.write.csv("valuation_growth_byzip.csv",header="true")

#by zip and by year categorization using percentiles
summary_yr=yearly_data.summary()
summary_yr.createOrReplaceTempView("summary_yr")
growth_1011_per25=spark.sql('SELECT 10_11growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1011_per50=spark.sql('SELECT 10_11growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1011_per75=spark.sql('SELECT 10_11growth from summary_yr WHERE summary="75%"').collect()[0][0]
growth_1112_per25=spark.sql('SELECT 11_12growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1112_per50=spark.sql('SELECT 11_12growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1112_per75=spark.sql('SELECT 11_12growth from summary_yr WHERE summary="75%"').collect()[0][0]
growth_1213_per25=spark.sql('SELECT 12_13growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1213_per50=spark.sql('SELECT 12_13growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1213_per75=spark.sql('SELECT 12_13growth from summary_yr WHERE summary="75%"').collect()[0][0]
growth_1314_per25=spark.sql('SELECT 13_14growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1314_per50=spark.sql('SELECT 13_14growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1314_per75=spark.sql('SELECT 13_14growth from summary_yr WHERE summary="75%"').collect()[0][0]
growth_1415_per25=spark.sql('SELECT 14_15growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1415_per50=spark.sql('SELECT 14_15growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1415_per75=spark.sql('SELECT 14_15growth from summary_yr WHERE summary="75%"').collect()[0][0]
growth_1516_per25=spark.sql('SELECT 15_16growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1516_per50=spark.sql('SELECT 15_16growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1516_per75=spark.sql('SELECT 15_16growth from summary_yr WHERE summary="75%"').collect()[0][0]
growth_1617_per25=spark.sql('SELECT 16_17growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1617_per50=spark.sql('SELECT 16_17growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1617_per75=spark.sql('SELECT 16_17growth from summary_yr WHERE summary="75%"').collect()[0][0]
growth_1718_per25=spark.sql('SELECT 17_18growth from summary_yr WHERE summary="25%"').collect()[0][0]
growth_1718_per50=spark.sql('SELECT 17_18growth from summary_yr WHERE summary="50%"').collect()[0][0]
growth_1718_per75=spark.sql('SELECT 17_18growth from summary_yr WHERE summary="75%"').collect()[0][0]
final_wcat_yr=spark.sql('SELECT boro, boro_name, postcode, 10_11growth, \
CASE WHEN 10_11growth<{0} THEN "poor" WHEN 10_11growth<{1} THEN "fair" WHEN 10_11growth<{2} THEN "good" ELSE "excellent" END 10_11_cat, \
11_12growth, CASE WHEN 11_12growth<{3} THEN "poor" WHEN 11_12growth<{4} THEN "fair" WHEN 11_12growth<{5} THEN "good" ELSE "excellent" END 11_12_cat, \
12_13growth, CASE WHEN 12_13growth<{6} THEN "poor" WHEN 12_13growth<{7} THEN "fair" WHEN 12_13growth<{8} THEN "good" ELSE "excellent" END 12_13_cat, \
13_14growth, CASE WHEN 13_14growth<{9} THEN "poor" WHEN 13_14growth<{10} THEN "fair" WHEN 13_14growth<{11} THEN "good" ELSE "excellent" END 13_14_cat, \
14_15growth, CASE WHEN 14_15growth<{12} THEN "poor" WHEN 14_15growth<{13} THEN "fair" WHEN 14_15growth<{14} THEN "good" ELSE "excellent" END 14_15_cat, \
15_16growth, CASE WHEN 15_16growth<{15} THEN "poor" WHEN 15_16growth<{16} THEN "fair" WHEN 15_16growth<{17} THEN "good" ELSE "excellent" END 15_16_cat, \
16_17growth, CASE WHEN 16_17growth<{18} THEN "poor" WHEN 16_17growth<{19} THEN "fair" WHEN 16_17growth<{20} THEN "good" ELSE "excellent" END 16_17_cat, \
17_18growth, CASE WHEN 17_18growth<{21} THEN "poor" WHEN 17_18growth<{22} THEN "fair" WHEN 17_18growth<{23} THEN "good" ELSE "excellent" END 17_18_cat \
from yearly_data'.format(growth_1011_per25,growth_1011_per50,growth_1011_per75,growth_1112_per25,growth_1112_per50,growth_1112_per75, \
growth_1213_per25,growth_1213_per50,growth_1213_per75,growth_1314_per25,growth_1314_per50,growth_1314_per75,growth_1415_per25, growth_1415_per50, \
growth_1415_per75,growth_1516_per25,growth_1516_per50,growth_1516_per75,growth_1617_per25,growth_1617_per50,growth_1617_per75, \
growth_1718_per25,growth_1718_per50,growth_1718_per75))

#save by zip and by year categorization using percentiles
final_yr_wcat_tosave=final_wcat_yr.coalesce(1)
final_yr_wcat_tosave.write.csv("valuation_yearly_growth_byzip.csv",header="true")